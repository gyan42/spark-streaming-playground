{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Optimization for Advances Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [RDD](#rdd)\n",
    "- [Partitions](#partitions)\n",
    "- [Joins](#joins)\n",
    "- [Serialisation](#dataserialisation)\n",
    "- [UDF](#udf)\n",
    "- [Analyse Execution plan](#executionplan)\n",
    "- [Data Skew](#data_skew)\n",
    "- [Cache](#cache)\n",
    "- [Storage](#storage)\n",
    "- [JVM](#jvm)\n",
    "- [Monitoring](#monitoring)\n",
    "- [Misc](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "\n",
    "* Avoid `groupBy` and prefer `reduceBy`\n",
    "\n",
    "**groupByKey**\n",
    "\n",
    "`groupByKey` can cause out of disk problems as data is sent over the network and collected on the reduce workers.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```scala\n",
    "sparkContext.textFile(\"hdfs://\")\n",
    "                    .flatMap(line => line.split(\" \") )\n",
    "                    .map(word => (word,1))\n",
    "                    .groupByKey()\n",
    "                    .map((x,y) => (x,sum(y)))\n",
    "```\n",
    "Eg:\n",
    "\n",
    "```scala\n",
    "  val a = sc.parallelize(List(\"dog\", \"cat\", \"owl\", \"gnu\", \"ant\"), 2)\n",
    "  val b = a.map(x => (x.length, x))\n",
    "  b.reduceByKey(_ + _).collect\n",
    "  // b.groupByKey().mapValues(_.mkString(\"\")).collect()\n",
    "  // Array[(Int, String)] = Array((3,dogcatowlgnuant))  \n",
    "```\n",
    "\n",
    "**reduceByKey**\n",
    "\n",
    "Data are combined at each partition, only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```scala\n",
    "sparkContext.textFile(\"hdfs://\")\n",
    "                    .flatMap(line => line.split(\" \"))\n",
    "                    .map(word => (word,1))\n",
    "                    .reduceByKey((x,y)=> (x+y))\n",
    "```\n",
    "\n",
    "```scala\n",
    "  val a = sc.parallelize(List(\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"), 2)\n",
    "  val b = a.map(x => (x.length, x))\n",
    "  b.reduceByKey(_ + _).collect\n",
    "  //// b.groupByKey().mapValues(_.mkString(\"\")).collect()\n",
    "  // Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))\n",
    "```\n",
    "\n",
    "**aggregateByKey:**\n",
    "\n",
    "same as reduceByKey, which takes an initial value.\n",
    "\n",
    "3 parameters as input i. initial value ii. Combiner logic iii. sequence op logic\n",
    "\n",
    "Eg:\n",
    "\n",
    "```scala\n",
    "val keysWithValuesList = Array(\"foo=A\", \"foo=A\", \"foo=A\", \"foo=A\", \"foo=B\", \"bar=C\", \"bar=D\", \"bar=D\")\n",
    "    val data = sc.parallelize(keysWithValuesList)\n",
    "    //Create key value pairs\n",
    "    val kv = data.map(_.split(\"=\")).map(v => (v(0), v(1))).cache()\n",
    "    val initialCount = 0;\n",
    "    val addToCounts = (n: Int, v: String) => n + 1\n",
    "    val sumPartitionCounts = (p1: Int, p2: Int) => p1 + p2\n",
    "    val countByKey = kv.aggregateByKey(initialCount)(addToCounts, sumPartitionCounts)\n",
    "//ouput: Aggregate By Key sum Results bar -> 3 foo -> 5\n",
    "```\n",
    "\n",
    "**combineByKey:**\n",
    "\n",
    "3 parameters as input\n",
    "\n",
    "Initial value: unlike aggregateByKey, need not pass constant always, we can pass a function that will return a new value.\n",
    "merging function\n",
    "combine function\n",
    "\n",
    "```scala\n",
    "Eg:\n",
    "val result = rdd.combineByKey(\n",
    "                        (v) => (v,1),\n",
    "                        ( (acc:(Int,Int),v) => acc._1 +v , acc._2 +1 ) ,\n",
    "                        ( acc1:(Int,Int),acc2:(Int,Int) => (acc1._1+acc2._1) , (acc1._2+acc2._2)) \n",
    "                        ).map( { case (k,v) => (k,v._1/v._2.toDouble) })\n",
    "result.collect.foreach(println)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='partitions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## Partitions - That Detemines the parallelism\n",
    "\n",
    "  * Avoid spills and utilize all the cores\n",
    "  * At input level\n",
    "    * `spark.default.parallelism`(don't use)\n",
    "    * `spark.sql.files.maxPartitionBytes`\n",
    "      * Use default partition size **128MB** , unless\n",
    "        * Increase parallelism\n",
    "        * Heavily nested/repeated data\n",
    "        * Generating data\n",
    "        * Source structure is not optimal\n",
    "        * UDFs\n",
    "      * ![](images/paritionfilesize.png)\n",
    "  * At shuffle level\n",
    "    * `sparl.sql.shuffle.partitions`\n",
    "    * Default maximum size is **2GB**\n",
    "\n",
    "  - At ouput level\n",
    "\n",
    "    - Coalesce to shrink number of partitions\n",
    "\n",
    "    - Repartition to increase the number of partitions\n",
    "\n",
    "    - ```python\n",
    "      df.write.option(\"maxRecordsPerFile\", n)\n",
    "      df.coalesce(n).write...\n",
    "      df.repartition(n).write...\n",
    "      df.repartition(n, [colA, ...]).write...\n",
    "      spark.sql.shuffle.partitions(n)\n",
    "      df.localCheckpoint(...).repartition(n).write...\n",
    "      df.localCheckpoint(...).coalesce(n).write...\n",
    "      ```\n",
    "\n",
    "    - ```\n",
    "      Write Data Size = 14.7GB\n",
    "      Desired File Size = 1500MB\n",
    "      Max write stage parallelism = 10\n",
    "      # Say we have 96 cores\n",
    "      96 – 10 == 86 cores idle during write\n",
    "      \n",
    "      14.7/96 cores = ~ 160 MB of parition size\n",
    "      spark.sql.files.maxPartitionBytes = 160 * 1024 * 1024\n",
    "      So now we use all 96 cores\n",
    "      ```\n",
    "\n",
    "    - ![](images/spark_partition_file_size.png)\n",
    "\n",
    "  - **Never rely on default partition size of `200` at any cost**\n",
    "\n",
    "    - ```\n",
    "      Say if you have data size of 210GB\n",
    "      number_partitions = 210000MB / 200MB = 1050\n",
    "      # irrespective of number of cores\n",
    "      spark.conf.set(\"Spark.sql.shuffle.paritions\", 1050)\n",
    "      # say if the cluster has 2000 cores\n",
    "      spark.conf.set(\"Spark.sql.shuffle.paritions\", 2000)\n",
    "      ```\n",
    "\n",
    "    - ![](images/spill.png)\n",
    "\n",
    "    - ![](images/spill_corrected.png)\n",
    "\n",
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"joins\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Joins\n",
    "\n",
    "  * SortMerge Join – Both sides are lrage\n",
    "\n",
    "  * **Broadcast DataFrame** Join when one side is small\n",
    "\n",
    "    * Default size is 10MB\n",
    "* Spark SQL uses **broadcast join** (aka **broadcast hash join**) instead of hash join to optimize join queries when the size of one side data is below [spark.sql.autoBroadcastJoinThreshold](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.autoBroadcastJoinThreshold).\n",
    "    * Broadcast allows to send a read-only variable cached on each node once, rather than sending a copy for all tasks. \n",
    "    * Automatic If:\n",
    "        (one side < spark.sql.autoBroadcastJoinThreshold) (default 10m)\n",
    "      * Risks\n",
    "        - Not Enough Driver Memory\n",
    "        - DF > spark.driver.maxResultSize\n",
    "        - DF > Single Executor Available Working Memory\n",
    "      * ![](images/broadcat_explained.png)\n",
    "    \n",
    "  * Requires to use Hive and its metastore for auto detection of table size\n",
    "    \n",
    "    ```scala\n",
    "    leftDF.join(broadcast(rightDF))\n",
    "    ```\n",
    "    \n",
    "  * **Put the Largest Dataset on the Left**\n",
    "  \n",
    "    * When you’re joining together two datasets where one is smaller than the other, put the larger dataset on the “Left”:\n",
    "    * When Spark shuffles data for the join, it keeps the data you specified on  the left static on the executors and transfers the data you designed on  the right between the executors. If the data that’s on the right, that’s being transferred, is larger, then the serialization and transfer of  the data will take longer.\n",
    "  \n",
    "  ```\n",
    "  val joinedDF = largerDF.leftJoin(smallerDF, largerDF(\"id\") === smallerDF(\"some_id\"))\n",
    "  ```\n",
    "    * Be aware of `null` values \n",
    "  \n",
    "        * Consider sample query where we are joining on highly null columns\n",
    "  \n",
    "        * ![](images/null_counts.png)\n",
    "  \n",
    "          ```sql\n",
    "          select * from  order_tbl orders left join customer_tbl  customer\n",
    "                on orders.customer_id = customer.customer_id\n",
    "                left join delivery_tbl delivery\n",
    "                on  orders.delivery_id = delivery.delivery_id\n",
    "          ```\n",
    "  \n",
    "        * In above query, 199/200 tasks would complete quite fast and then probably gets stuck on the last task. \n",
    "  \n",
    "        * ***Reason for the above behavior:\\*** Let’s say that we have asked Spark to join two DataFrames — TABLE1 and  TABLE2. When Spark executes this code, internally it performs the  default Shuffle Hash Join (Exchange hashpartitioning).\n",
    "  \n",
    "          ```\n",
    "          +- Exchange hashpartitioning(delivery_id#22L, 400)\n",
    "                         +- *(6) Project [delivery_id#22L]\n",
    "          ```\n",
    "  \n",
    "          In this process, Spark hashes the join column and sorts it. And then it  tries to keep the records with same hashes in both partitions on the  same executor hence all the null values of the table will go to one  executor and spark gets into a continuous loop of shuffling and garbage  collection with no success.\n",
    "  \n",
    "      * Solution is split the tables into two parts, one with null values(i.e all the values) and without null values. \n",
    "  \n",
    "      * Use table without null values for join and then union them with full data\n",
    "  \n",
    "      * This way null values won't participate in the join\n",
    "  \n",
    "      * ```sql\n",
    "        CREATE TABLE order_tbl_customer_id_not_null as select * from order_tbl where customer_id is not null;\n",
    "        \n",
    "        CREATE TABLE order_tbl_customer_id_null as select * from order_tbl where customer_id is null;\n",
    "        \n",
    "        --Rewrite query\n",
    "        select orders.customer_id\n",
    "        from  order_tbl_customer_id_not_null orders left join customer_tbl  customer \n",
    "                 on orders.customer_id = customer.customer_id\n",
    "        union all\n",
    "        select ord.customer_id from order_tbl_customer_id_null ord;\n",
    "        ```\n",
    "        \n",
    "    * Coalesce vs repartition\n",
    "\n",
    "      In the literature, it’s often mentioned that *coalesce* should be preferred over *repartition* to reduce the number of partitions because it avoids a shuffle step in some cases.\n",
    "\n",
    "      But ***coalesce\\* has some limitations** (outside the scope of this article): it cannot increase the number of partitions and may generate skew partitions.\n",
    "\n",
    "      Here is one case where a **repartition should be preferred**. In this case, we filter most of a dataset.\n",
    "\n",
    "      ```\n",
    "      df.doSth().coalesce(10).write(…)\n",
    "      ```\n",
    "\n",
    "\n",
    "\n",
    "      ![img](images/coalesce.png)\n",
    "\n",
    "      The good point about *coalesce* is that it avoids a shuffle. However, all the computation is done by only 10 tasks.\n",
    "\n",
    "      This is due to the fact that **the number of tasks depends on the number of partitions of the output of the stage**, each one computing a big bunch of data. So a maximum of 10 nodes will perform the computation.\n",
    "\n",
    "      ```\n",
    "      df.doSth().repartition(10).write(…)\n",
    "      ```\n",
    "\n",
    "      ![](images/repartition.png)\n",
    "\n",
    "      Using *repartition* we can see that the total duration is way shorter (a few seconds  instead of 31 minutes). The filtering is done by 200 tasks, each one  working on a small subset of data. It’s also way smoother from a memory  point a view, as we can see in the graph below.\n",
    "\n",
    "      ![](images/repartition_memory.png)\n",
    "  \n",
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataserialisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Serialisation\n",
    "\n",
    "  - It is the process of converting the in-memory object to another format  that can be used to store in a file or send over the network.\n",
    "  - Two options\n",
    "    - Java serialization\n",
    "    - [Kryo serialization](https://github.com/EsotericSoftware/kryo)\n",
    "  - Configuration : “spark.serializer” to “org.apache.spark.serializer.KyroSerializer\"\n",
    "\n",
    "```scala\n",
    " val conf = new SparkConf().setAppName(...).setMaster(...)\n",
    "      .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .set(\"spark.kryoserializer.buffer.max\", \"128m\")\n",
    "      .set(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "      .registerKryoClasses(\n",
    "        Array(classOf[ArrayBuffer[String]], classOf[ListBuffer[String]])\n",
    "      )\n",
    "``` \n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"udf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## UDF\n",
    "\n",
    "\n",
    "  * Avoid User defined Function **UDFs** and User Defined Aggregate Funtions **UDAF** \n",
    "  * Prefer in build SQL Functions where ever possible\n",
    "  * Traditional UDFs cannot use Tungsten\n",
    "\n",
    "    * Use `org.apache.spark.sql.functions`\n",
    "    * Use PandasUDFs\n",
    "\n",
    "      * Utilizes Apache Arrow\n",
    "    * Use SparkR UDFs\n",
    "  * https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "  * https://fr.slideshare.net/cfregly/advanced-apache-spark-meetup-project-tungsten-nov-12-2015\n",
    "\n",
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"executionplan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Analyse Execution plan\n",
    "\n",
    "  - Stages are created when there is a need for shuffle for operations like `join` or `groupBy`\n",
    "\n",
    "  - **Number of stages**\n",
    "\n",
    "    * Thought the Spark engine does pretty good job of optimizing the DAGs for executions, it is also developer responsibility to keep the number of stages under a reasonable number.\n",
    "    * This involves good amount of understanding of APIs and SQL query optimization\n",
    "\n",
    "  - We use the` .explain(true)` command to show the execution plan detailing all the steps (stages) involved for a job. Here is an example:\n",
    "\n",
    "    ![](images/explain.png)\n",
    "    \n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_skew\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Data Skew\n",
    "\n",
    "  - Common symptoms of data skew are:\n",
    "    - Frozen stages and tasks.\n",
    "      - Especially with the last couple of tasks in a stage\n",
    "    - Low utilization of CPU.\n",
    "    - Out of memory errors.\n",
    "  - Data broadcast during join operation can help with minimize the data skew side effects. Increase the `spark.sql.autoBroadcastJoinThreshold` for Spark to consider tables of bigger size. Make sure enough memory is available in driver and executors\n",
    "  - Salting\n",
    "    - In a SQL join operation, the join key is changed to redistribute data in an even manner so that processing for a partition does not take more  time. This technique is called salting.\n",
    "    \n",
    "    -  Add Column to each side with random int between 0 and `spark.sql.shuffle.partitions` – 1 to both sides\n",
    "    \n",
    "    - Add join clause to include join on generated column above\n",
    "    \n",
    "    - Drop temp columns from result\n",
    "    \n",
    "    - ```scala\n",
    "      df.groupBy(“city”, “state”).agg(<f(x)>).orderBy(col.desc)\n",
    "      val saltVal = random(0, spark.conf.get(org...shuffle.partitions) - 1)\n",
    "      df.withColumn(“salt”, lit(saltVal))\n",
    "      .groupBy(“city”, “state”, “salt”)\n",
    "      .agg(<f(x)>)\n",
    "      .drop(“salt”)\n",
    "      .orderBy(col.desc)\n",
    "      \n",
    "      ```\n",
    "      \n",
    "* **Highly imbalanced dataset**\n",
    "\n",
    "  - To quickly check if everything is ok we review the execution duration of each task and look for **heterogeneous process time**. If one of the tasks is significantly slower than the others it will  extend the overall job duration and waste the resources of the fastest  executors.\n",
    "\n",
    "    ![](images/stage_execution_time.png)\n",
    "    \n",
    "\n",
    "- **COMPUTE STATISTICS** of Tables Before Processing\n",
    "\n",
    "  - Before querying a series of tables, it can be helpful to tell spark to Compute the Statistics of those tables so that the Catalyst Optimizer can come  up with a better plan on how to process the tables.\n",
    "\n",
    "  ```\n",
    "  spark.sql(\"ANALYZE TABLE dbName.tableName COMPUTE STATISTICS\")\n",
    "  ```\n",
    "\n",
    "  - In some cases, Spark doesn’t get everything it needs from just the above  broad COMPUTE STATISTICS call. It also helps to tell Spark to check  specific columns so the Catalyst Optimizer can better check those  columns. It’s recommended to COMPUTE STATISTICS for any columns that are involved in filtering and joining.\n",
    "\n",
    "```\n",
    "spark.sql(\"ANALYZE TABLE dbName.tableName COMPUTE STATISTICS FOR COLUMNS joinColumn, filterColumn\")\n",
    "```\n",
    "\n",
    "- Avoid calling **DataFrame/RDD `count` API** unless it is absolutely necessary. \n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cache\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Cache\n",
    "\n",
    "* **Inappropriate use of caching**\n",
    "\n",
    "  There is no universal answer when choosing what should be cached. Caching an intermediate result can **dramatically improve performance** and it’s tempting to cache a lot of things. However, due to Spark’s  caching strategy (in-memory then swap to disk) the cache can end up in a slightly slower storage. Also, using that storage space for caching  purposes means that it’s not available for processing. In the end,  caching might cost more than simply reading the *DataFrame*.\n",
    "\n",
    "  In the *Storage* tab of the UI we verify the *Fraction Cached* and also look at the *Size in Memory* and *Size on Disk* distribution.\n",
    "\n",
    "  ![](images/cacahe.png)\n",
    "  \n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"storage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Storage\n",
    "\n",
    "* **Pitfalls with AWS S3**\n",
    "\n",
    "  - A simple renaming actually needs to copy and then delete the original file.\n",
    "\n",
    "  - The first [workaround](https://medium.com/@subhojit20_27731/apache-spark-and-amazon-s3-gotchas-and-best-practices-a767242f3d98) when using Spark with S3 as an output it to use this specific configuration:\n",
    "\n",
    "    ```shell\n",
    "    spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2 spark.speculation false\n",
    "    ```\n",
    "    \n",
    "- **Data Skipping**\n",
    "  \n",
    "  - **Parquet**\n",
    "    \n",
    "    - Columnar storage format\n",
    "    - Spark can leverage **predicate push down** (i.e. pushing down the filtering closer to the data).\n",
    "    - However, predicate pushdown should be used with extra care. Even if it appears in the execution plan, **it will not always be effective**. For example, a filter push-down does not work on String and Decimal data types (cf[ PARQUET-281](https://issues.apache.org/jira/browse/PARQUET-281)).\n",
    "    \n",
    "  - Hive Partitions\n",
    "  \n",
    "  - Hive **Bucketing**\n",
    "  \n",
    "    * Consider when there is a high hashable column values\n",
    "  \n",
    "    * `spark.sql.sources.bucketing.enabled=True`\n",
    "  \n",
    "    * Set  `hive.enforce.bucketing=false` and `hive.enforce.sorting=false`\n",
    "  \n",
    "    * https://www.slideshare.net/databricks/hive-bucketing-in-apache-spark-with-tejas-patil\n",
    "  \n",
    "    * ```scala\n",
    "      df.write\n",
    "        .bucketBy(numBuckets, \"col1\", ...)\n",
    "        .sortBy(\"col1\", ...)\n",
    "        .saveAsTable(\"/path/table_name\")\n",
    "      ```\n",
    "  \n",
    "    * ```sql\n",
    "      create table table_name(col1 INT, ...)\n",
    "      \tusing parquet\n",
    "      \tCLUSTERED By (col1, ...)\n",
    "      \tSORTED BY(col1, ...)\n",
    "      \tINTO `numBuckets` BUCKETS\n",
    "      ```\n",
    "  \n",
    "    * ![](images/bucketing.png)\n",
    "    \n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"jvm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Java Garbage Collection\n",
    "\n",
    "To more on the topic read [here](https://javarevisited.blogspot.com/2011/04/garbage-collection-in-java.html)\n",
    "\n",
    "<img src=\"images/JavaGarbageCollectionheap.png\" />\n",
    "\n",
    "`spark.executor.extraJavaOptions = -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:UseG1GC XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThread=20`\n",
    "\n",
    "  - Analyse the logs for the memory usage, most likely the problem would be with the data partitions.\n",
    "  - https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning\n",
    "  - EMR : https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-debugging.html\n",
    "  - Tool to analyse the logs: https://gceasy.io/\n",
    "\n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"monitoring\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Monitoring spark applications\n",
    "\n",
    "  - Spark includes a configurable metrics system based on the [*dropwizard.metrics*](http://metrics.dropwizard.io/) library. It is set up via the Spark configuration. As we already are heavy users of **Graphite** and **Grafana**, we use the provided [Graphite sink](https://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/).\n",
    "\n",
    "    The Graphite sink needs to be **used with caution**. This is due to the fact that, for each metric, Graphite creates a fixed-size database to store data points. These zeroed-out [*Whisper*](https://graphite.readthedocs.io/en/latest/whisper.html) files consume quite a lot of disk space.\n",
    "\n",
    "    By default, the application id is added to the namespace, which means that every *spark-submit* operation creates a new metric. Thanks to [SPARK-5847](https://issues.apache.org/jira/browse/SPARK-5847) it is now possible to **mitigate the \\*Whisper\\* behavior** and remove the *spark.app.id* from the namespace.\n",
    "\n",
    "  ```\n",
    "  \tspark.metrics.namespace=$name\n",
    "  ```\n",
    "\n",
    "  \n",
    "  - https://github.com/hammerlab/grafana-spark-dashboards\n",
    "  - https://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/\n",
    "  - https://github.com/qubole/sparklens\n",
    "\n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"misc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Misc \n",
    "\n",
    "* Try alternatives for AWS EMR with plain EC2 \n",
    "\n",
    "  - https://github.com/nchammas/flintrock\n",
    "  - https://heather.miller.am/blog/launching-a-spark-cluster-part-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Good reads\n",
    "\n",
    "- https://medium.com/teads-engineering/lessons-learned-while-optimizing-spark-aggregation-jobs-f93107f7867f\n",
    "- https://www.slideshare.net/databricks/apache-spark-coredeep-diveproper-optimization\n",
    "- https://michalsenkyr.github.io/2018/01/spark-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the respective Gist for this notebook :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/Mageswaran1989/e1957c887cd8ec900f84ae91842636b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mageswarand/anaconda3/envs/vh/bin/jupyter-nbconvert\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/mageswarand/anaconda3/envs/vh/lib/python3.6/site-packages/jupyter_core/application.py\", line 270, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/home/mageswarand/anaconda3/envs/vh/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/mageswarand/anaconda3/envs/vh/lib/python3.6/site-packages/nbconvert/nbconvertapp.py\", line 340, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"/home/mageswarand/anaconda3/envs/vh/lib/python3.6/site-packages/nbconvert/nbconvertapp.py\", line 499, in convert_notebooks\n",
      "    cls = get_exporter(self.export_format)\n",
      "  File \"/home/mageswarand/anaconda3/envs/vh/lib/python3.6/site-packages/nbconvert/exporters/base.py\", line 113, in get_exporter\n",
      "    % (name, ', '.join(get_export_names())))\n",
      "ValueError: Unknown exporter \"ltex\", did you mean one of: asciidoc, custom, html, html_ch, html_embed, html_toc, html_with_lenvs, html_with_toclenvs, latex, latex_with_lenvs, markdown, notebook, pdf, python, rst, script, selectLanguage, slides, slides_with_lenvs?\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to ltex SparkOptimization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'e1957c887cd8ec900f84ae91842636b9'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 36 (delta 2), reused 25 (delta 1), pack-reused 9\u001b[K\n",
      "Unpacking objects: 100% (36/36), done.\n"
     ]
    }
   ],
   "source": [
    "! rm -rf e1957c887cd8ec900f84ae91842636b9\n",
    "! git clone https://gist.github.com/Mageswaran1989/e1957c887cd8ec900f84ae91842636b9\n",
    "! cd e1957c887cd8ec900f84ae91842636b9 && rm *\n",
    "! cp SparkOptimization.html e1957c887cd8ec900f84ae91842636b9/SparkOptimization.html\n",
    "! cd e1957c887cd8ec900f84ae91842636b9 && git add . && git commit -m \"updated\" && git push origin HEAD\n",
    "! rm -rf e1957c887cd8ec900f84ae91842636b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master bedc1c1] updated\n",
      " 21 files changed, 14003 insertions(+), 1592 deletions(-)\n",
      " delete mode 100644 JavaGarbageCollectionheap.png\n",
      " create mode 100644 SparkOptimization.html\n",
      " delete mode 100644 SparkOptimization.ipynb\n",
      " delete mode 100644 broadcat_explained.png\n",
      " delete mode 100644 bucketing.png\n",
      " delete mode 100644 cacahe.png\n",
      " delete mode 100644 coalesce.png\n",
      " delete mode 100644 dependencies.png\n",
      " delete mode 100644 driver_tasks.png\n",
      " delete mode 100644 explain.png\n",
      " delete mode 100644 null_counts.png\n",
      " delete mode 100644 paritionfilesize.png\n",
      " delete mode 100644 repartition.png\n",
      " delete mode 100644 repartition_memory.png\n",
      " delete mode 100644 schedule-process.png\n",
      " delete mode 100644 spark_execution_model.png\n",
      " delete mode 100644 spark_optimization_for_Advanced_users.ipynb\n",
      " delete mode 100644 spark_partition_file_size.png\n",
      " delete mode 100644 spill.png\n",
      " delete mode 100644 spill_corrected.png\n",
      " delete mode 100644 stage_execution_time.png\n",
      "Counting objects: 3, done.\n",
      "Delta compression using up to 12 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 2.16 MiB | 252.00 KiB/s, done.\n",
      "Total 3 (delta 0), reused 0 (delta 0)\n",
      "To https://gist.github.com/Mageswaran1989/e1957c887cd8ec900f84ae91842636b9\n",
      "   d84b8d7..bedc1c1  HEAD -> master\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook SparkOptimization.ipynb to html\n",
      "[NbConvertApp] Writing 324153 bytes to SparkOptimization.html\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
